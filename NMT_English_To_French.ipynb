{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_English_To_French.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvS3PZZsqykXIuC5uDribz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5605b939ee847a9a5c4a1e2078ed131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c9a5e2a8a7ae4253a7db9fda9e6c1db8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee8c194a93104e33b15d925df202c9ce",
              "IPY_MODEL_aa22c08e7bff4ed19391b677ffaa2732",
              "IPY_MODEL_2f8bfad53a974023ab5d0a7101445ce6"
            ]
          }
        },
        "c9a5e2a8a7ae4253a7db9fda9e6c1db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee8c194a93104e33b15d925df202c9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c4bda9e63ff46b3af01875842da2bdf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a200ccd3960f4a0eb34bfcfbf8b10e52"
          }
        },
        "aa22c08e7bff4ed19391b677ffaa2732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dbf3dabc44e4499e90918a5c3006f72d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73de936feaaa496d868a668bd0e4c431"
          }
        },
        "2f8bfad53a974023ab5d0a7101445ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5e12b7d8ebef4b5282acc54463cf0f41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 134/? [01:09&lt;00:00,  1.89it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df6f6ef2b9c24681a3965ceda51d882e"
          }
        },
        "9c4bda9e63ff46b3af01875842da2bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a200ccd3960f4a0eb34bfcfbf8b10e52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbf3dabc44e4499e90918a5c3006f72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73de936feaaa496d868a668bd0e4c431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e12b7d8ebef4b5282acc54463cf0f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df6f6ef2b9c24681a3965ceda51d882e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pSN0W/Transformers/blob/main/NMT_English_To_French.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation using Transformers\n"
      ],
      "metadata": {
        "id": "FW3Sw8S9qTpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project I will implement transformer from scratch and then use it for neural machine translation to convert english to french"
      ],
      "metadata": {
        "id": "Loyh4cEZqd3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dependencies"
      ],
      "metadata": {
        "id": "vQRQVwpaq0OW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iQqiYT12MDr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import tqdm.notebook as tq\n",
        "import time\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "Nqwqc9SRs-qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n"
      ],
      "metadata": {
        "id": "Lhs1s137tRNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading files"
      ],
      "metadata": {
        "id": "XD3_BV03tWgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the drive first\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGzB0tdztVvM",
        "outputId": "8723b169-f827-4bc8-c953-23b576933071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Give a base dir for your files\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/NMT_En_to_Fr'"
      ],
      "metadata": {
        "id": "qzdbuxhzt5ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing a function to read from file and return the data in it\n",
        "\n",
        "def read_data_from_file(file_path):\n",
        "    with open(file_path) as f:\n",
        "        data = f.read()\n",
        "    return data"
      ],
      "metadata": {
        "id": "YM4BQyWZuTlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from the files in variable\n",
        "\n",
        "# Data that will be used for translation\n",
        "europal_en = read_data_from_file(os.path.join(BASE_DIR,'europarl-v7.fr-en.en'))\n",
        "europal_fr = read_data_from_file(os.path.join(BASE_DIR,'europarl-v7.fr-en.fr'))\n",
        "\n",
        "# Frefixes that ends with '.' but doesn't necessarily mean they are end of sentence like a.m.\n",
        "non_breaking_prefixes_en = read_data_from_file(os.path.join(BASE_DIR,'non_breaking.en'))\n",
        "non_breaking_prefixes_fr = read_data_from_file(os.path.join(BASE_DIR,'non_breaking.fr'))"
      ],
      "metadata": {
        "id": "1d1YeZb-u5_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Data\n"
      ],
      "metadata": {
        "id": "4wSq3TlrwQXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The non_breaking prefixes in the file are stored in the form of one non_breaking prefix per line\n",
        "# additionaly they don't end with '.' for example it is stored as 'Mr' not 'Mr.'\n",
        "\n",
        "# convert 'Mr' to ' Mr.'\n",
        "\n",
        "non_breaking_prefixes_en_list = [' '+prefix+'.' for prefix in non_breaking_prefixes_en.split('\\n')]\n",
        "non_breaking_prefixes_fr_list = [' '+prefix+'.' for prefix in non_breaking_prefixes_fr.split('\\n')]\n",
        "\n",
        "non_breaking_prefixes_en_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh4wXuwawS9L",
        "outputId": "8627680a-5dc4-4d53-d204-3de45c5c533c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' A.', ' B.', ' C.', ' D.', ' E.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean the data \n",
        "\n",
        "# we first replace all the non_breaking '.' with '.###'\n",
        "# and then at the end replace '.###' with ' '\n",
        "\n",
        "def clean_corpus(corpus):\n",
        "\n",
        "    # replace the non ending prefixes ' Mr.' -> ' Mr.###'\n",
        "    for prefix in non_breaking_prefix_en_list+non_breaking_prefix_fr_list:\n",
        "        corpus = corpus.replace(prefix,prefix+'###')\n",
        "\n",
        "    # replace the '.' not followed by space with '.###'\n",
        "    corpus = re.sub(r\"\\.(?=[A-Z]||[a-z]||[0-9])\",'.###',corpus)\n",
        "\n",
        "    # replace '.###' with ' '\n",
        "    corpus = re.sub(\".###\",' ',corpus)\n",
        "\n",
        "    # replace more than one spaces with a single one\n",
        "    corpus = re.sub('  +',' ',corpus)\n",
        "\n",
        "    # return a list of clean corpus\n",
        "    return corpus.split('\\n')\n"
      ],
      "metadata": {
        "id": "u5ZlCHZ_x1zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the english and french corpus\n",
        "\n",
        "corpus_en = clean_corpus(europal_en)\n",
        "corpus_fr = clean_corpus(europal_fr)"
      ],
      "metadata": {
        "id": "3lXOjHk32EyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "v5sqwL1Y4GLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using SubwordTextTokenizer as it helps us in avoiding the problem of OOV tokens by breaking them into subwords and then using them for encoding"
      ],
      "metadata": {
        "id": "Alv3n9Tb4Lic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build your tokenizer\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en,target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_fr,target_vocab_size=2**13)\n",
        "\n",
        "# save the tokenizer to a file\n",
        "tokenizer_en.save_to_file(os.path.join(BASE_DIR,'tokenier_en'))\n",
        "tokenizer_fr.save_to_file(os.path.join(BASE_DIR,'tokenier_fr'))"
      ],
      "metadata": {
        "id": "LRXwfl_j4fVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the tokenizer to tokenize the data\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(os.path.join(BASE_DIR,'tokenier_en'))\n",
        "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.load_from_file(os.path.join(BASE_DIR,'tokenier_fr'))\n",
        "\n",
        "# set vocab size\n",
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size+2 # 2 for <sos> and <eos> token\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size+2\n"
      ],
      "metadata": {
        "id": "UadgWCny-G3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the data\n",
        "# using VOCAB_SIZE-2 for <sos> and VOCAB_SIZE-1 for <eos>\n",
        "inputs = [[VOCAB_SIE_EN-2]+tokenizer_en.encode(sentence)+[VOCAB_SIZE_EN-1] for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIE_FR-2]+tokenizer_fr.encode(sentence)+[VOCAB_SIZE_FR-1] for sentence in corpus_fr]"
      ],
      "metadata": {
        "id": "OgaJRccupiQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the cdf of length\n",
        "\n",
        "def plot_cdf_for_length_of_sequence(sequence):\n",
        "    length_of_sequences = [len(sentence) for sentence in sequence]\n",
        "    print(np.percentile(length_of_sequences,[50,60,70,80,85,90,95,100]))\n",
        "    sns.ecdfplot(data=length_of_sequences)"
      ],
      "metadata": {
        "id": "2JmMJDKSAo-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cdf_for_length_of_sequence(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "mMESw0qIBrvN",
        "outputId": "0cce7328-7b52-4a18-dd5f-5d3e3d2311aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  29.   33.   39.   45.   50.   57.   68. 1221.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cdf_for_length_of_sequence(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "M26PJz6oCI0x",
        "outputId": "47f38724-7101-4446-f23a-eea4ae353a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  34.   39.   45.   54.   59.   67.   80. 1420.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Long Sentances"
      ],
      "metadata": {
        "id": "JKuqMLEsCxiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From the graph and percetile it looke like max_length=40 will be good but due to computational constraint\n",
        "# we will use max length of sequence as 20\n",
        "\n",
        "MAX_LENGTH = 20"
      ],
      "metadata": {
        "id": "SDmzeSK3C0e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will remove sequences greater than MAX_LENGTH\n",
        "\n",
        "# get the index to remove\n",
        "idx_to_remove = [idx for idx,sentance in enumerate(inputs) if len(sentance)>MAX_LENGTH]\n",
        "\n",
        "# remove those indexes\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# get the index to remove\n",
        "idx_to_remove = [idx for idx,sentance in enumerate(outputs) if len(sentance)>MAX_LENGTH]\n",
        "\n",
        "# remove those indexes\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "metadata": {
        "id": "4On6XwlQDfdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pad Sequences"
      ],
      "metadata": {
        "id": "VO8Vwlx8ElB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will pad the input and output for them to be of the same size\n",
        "\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                      value=0,\n",
        "                                                      padding=\"post\",\n",
        "                                                      maxlen=MAX_LENGTH\n",
        "                                                    )\n",
        "\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                      value=0,\n",
        "                                                      padding=\"post\",\n",
        "                                                      maxlen=MAX_LENGTH\n",
        "                                                    )"
      ],
      "metadata": {
        "id": "xKfQyWTTEoB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting to tensorflow dataset"
      ],
      "metadata": {
        "id": "Bw0NPWkMGnp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining batchsize and buffer size \n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000"
      ],
      "metadata": {
        "id": "n5XU8q-1GsZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensorflow dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs,outputs))\n"
      ],
      "metadata": {
        "id": "b9OXJ0QWG7VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the dataset\n",
        "tf.data.experimental.save(dataset,os.path.join(BASE_DIR,'processed_data'))"
      ],
      "metadata": {
        "id": "w0j4_v8GH1zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved data\n",
        "dataset = tf.data.experimental.load(os.path.join(BASE_DIR,'processed_data'))\n",
        "\n",
        "# cache it to improve performance\n",
        "dataset = dataset.cache()\n",
        "\n",
        "# shuffle it to increase robustness\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# prefetch it to improve performance\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "1xPtcfpRIKoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model "
      ],
      "metadata": {
        "id": "a0AcA6iqJAqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding"
      ],
      "metadata": {
        "id": "L7eFj4Z-JGaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the transformer are unable to get information on the position of the word in the sequence we add this information ourself by creating Positional Encoding using the formula<br>\n",
        "<h3 align=\"center\"> $$ PE_{(pos,2i)} = sin(\\frac{pos}{10000^{2i/d_{model}}})$$ </h3>\n",
        "<h3 align=\"center\"> $$ PE_{(pos,2i+1)} = cos(\\frac{pos}{10000^{2i/d_{model}}})$$ </h3><br>\n",
        "<h4 align=\"justify\"> pos = Position of the word in sequence <br>\n",
        "d<sub>model</sub> = Embedding dimension of the word vector<br>\n",
        "i = We will create the position embedding for a word of length d<sub>model</sub>.  Now here i is the index in of the Position Embedding that means if d<sub>model</sub> = 4 i = [0,1,2,3]\n",
        "<h4>\n"
      ],
      "metadata": {
        "id": "BH8T9ge9jzVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PositionalEmbedding,self).__init__()\n",
        "\n",
        "    def get_angles(self,pos,i,d_model):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            pos     = An array corresponding to all the word indices [0,d_model)            shape = (seq_len,1)\n",
        "            i       = indexes for Positional Embedding Vector for a position in dequence    shape = (1,d_model)\n",
        "            d_model = dimensions of the word embedding\n",
        "        Returns:\n",
        "            All the angles corresponding to the positional encoding                         shape = (seq_len,d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        # 10000^(2*i/d_model)\n",
        "        # Here we will use the formula 10000^((2*(i//2))/d_model) instead as we are calculating PE for (pos,i)\n",
        "        # we add minus sign case we want the power to be -1 to equate 1/{value of expression}\n",
        "        denominator_of_angle = np.power(10000,(-2*(i//2))/np.float32(d_model))\n",
        "\n",
        "        # once we have converted each of the position in the PE vector to required form\n",
        "        # next what we need to do is to multipy pos with the denominator\n",
        "        # for this we will take advantage of broadcasting in nummpy\n",
        "        angles = pos * denominator_of_angle\n",
        "\n",
        "        return angles\n",
        "\n",
        "    def call(self,embedded_inputs):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            embedded_inputs = This layer gets input from the embedding layer    shape = (batch_size,seq_len,d_model)\n",
        "        Returns:\n",
        "            The new embedding after adding positional embedding to word embedding \n",
        "        \"\"\"\n",
        "\n",
        "        seq_length = embedded_inputs.shape.as_list()[-2]\n",
        "        d_model    = embedded_inputs.shape.as_list()[-1]\n",
        "\n",
        "        # getting position and i vector\n",
        "        pos = np.arange(seq_length).reshape(seq_length,1)\n",
        "        i   = np.arange(d_model).reshape(1,d_model)\n",
        "\n",
        "        # getting the angle from those vectors\n",
        "        angles = self.get_angles(pos=pos,i=i,d_model=d_model)\n",
        "\n",
        "        # now we need to apply sin on all the even index of sequence and cos on the odd\n",
        "        # we take all the sequence and the to apply sin start from begining and take a jump of 2\n",
        "        # for cos we start from first index and then take a jump of 2\n",
        "        angles[:,::2]  = np.sin(angles[:,::2])\n",
        "        angles[:,1::2] = np.cos(angles[:,1::2])\n",
        "\n",
        "        # we add new axis to angles to make it able to sum with embedded_inputs '...' means keep everything be\n",
        "        pos_encoding = angles[np.newaxis,...]\n",
        "\n",
        "        # we add positional embedding with word embedding to create new embedding\n",
        "        new_embedding = embedded_inputs+tf.cast(pos_encoding,tf.float32)\n",
        "\n",
        "        return new_embedding\n"
      ],
      "metadata": {
        "id": "3Ubk1c-cJMO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Computation"
      ],
      "metadata": {
        "id": "UY601Pji-AVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaled Dot Product Attention"
      ],
      "metadata": {
        "id": "AXQUuMYh-PWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most basic type of attention is dot product attention. Here we multipy the key vector by query vector first to know the dependence and then softmax is taken to have a probabilistic interpretation of things. Here we also use a scaling factor of ${\\sqrt{D_k}}$ which is dimension of the keys to avoid gradient explosion problem <br>\n",
        "<h3> Simply put\n",
        "$$ Attention(Q,K,V) = softmax({\\frac{QK^T}{ \\sqrt{D_k}}})V $$\n",
        "</h3>"
      ],
      "metadata": {
        "id": "q56N_OaL-VWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following image display dot product attention <br>\n",
        "The input are first passed through linear layers to compute Q,K,V vectot and then attention is calculated\n",
        "\n",
        "<p align=\"center\"><img src=\"https://miro.medium.com/max/640/1*kxR_DjBgFw7LTTN-Ut34Pw.gif\" ></p>\n"
      ],
      "metadata": {
        "id": "P2JoYVwdBUYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query,key,value,mask):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        query = The query vector we get after passing through liner layer       shape = (batch_size,num_head,seq_len,dim_head)\n",
        "        key   = The key   vector we get after passing through liner layer       shape = (batch_size,num_head,seq_len,dim_head)\n",
        "        value = The value vector we get after passing through liner layer       shape = (batch_size,num_head,seq_len,dim_head)\n",
        "        mask  = A optional matrix to apply mask before computing softmax        shape = (batch_size,1,seq_len,1)\n",
        "    Returns:\n",
        "        Attention(Q,K,V) as defined by the above formula\n",
        "    \"\"\"\n",
        "\n",
        "    # We find our Dk and convert it to float to avoid compications on divison\n",
        "    keys_dim = tf.cast(tf.shape(key)[-1],tf.float32)\n",
        "\n",
        "    # Now we find the attention score by doing Q@K.T\n",
        "    attention_score = tf.matmul(query,key,transpose_b=True)\n",
        "\n",
        "    # Now we do scaling of the attention score\n",
        "    scaled_attention_score = attention_score/tf.math.sqrt(keys_dim)\n",
        "\n",
        "    # we apply mask if mask is available\n",
        "    if mask is not None:\n",
        "        scaled_attention_score += (mask*-1e9)\n",
        "\n",
        "    # we find the attention weight by applying softmax\n",
        "    attention_weight = tf.nn.softmax(scaled_attention_score,axis=-1)\n",
        "\n",
        "    # find attention by multiplying it with value\n",
        "    attention = tf.matmul(attention_weight,value)\n",
        "\n",
        "    return attention\n"
      ],
      "metadata": {
        "id": "N_Y-4TdGB9jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiheaded attention\n"
      ],
      "metadata": {
        "id": "P_AjapdsFyK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While calculating attention in transformer we break the query key and value vector int multiple subvectors and then pass all these subvectors through different attention calculator (known as heads) and then after attention is done for all the heads we concatenate them to get our final vector.\n",
        "\n",
        "Simply put <br>\n",
        "Queries, Keys and Values vector are broken in multiple projections based on their last dimensions and then each of these subvectors are passed through multiple heads for attention calculation and then at the end they are concatenated.\n",
        "\n",
        "<p align=\"center\"> <img src=\"https://miro.medium.com/max/640/0*X0c962yMhgRKfMTD.gif\"> </p>\n",
        "\n",
        "At the end we pass it through linear layer. The below image can help us to understand the architecture of multi head attention better\n",
        "<p align=\"center\"> <img src=\"https://miro.medium.com/max/700/0*4Qos9ymoz4LW1pmP.png\"> </p>\n"
      ],
      "metadata": {
        "id": "z8PIjpeSG1n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(layers.Layer):\n",
        "    def __init__(self,num_heads):\n",
        "        \"\"\"\n",
        "            num_proj = number of projection(or heads)\n",
        "        \"\"\"\n",
        "        super(MultiHeadedAttention,self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        # Embeding dimension\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # make sure that embedding dimension is completely divisible by num_heads\n",
        "        assert self.d_model%self.num_heads == 0\n",
        "\n",
        "        # last dimension after breaking\n",
        "        self.dim_heads  = self.d_model//self.num_heads\n",
        "        # declare linear layers to pass our input through\n",
        "        self.linear_k = layers.Dense(units=self.d_model)\n",
        "        self.linear_q = layers.Dense(units=self.d_model)\n",
        "        self.linear_v = layers.Dense(units=self.d_model)\n",
        "\n",
        "        # Final dense layer to pass the vectors after concatenation\n",
        "        self.linear_fin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    def split_mat(self,input,batch_size):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            input      : The matrix we want to split    shape = (batch_size,seq_length,d_model)\n",
        "        Returns:\n",
        "            tensor after breaking it into parts         shape = (batch_size,seq_length,num_heads,dim_heads)\n",
        "        \"\"\"\n",
        "\n",
        "        # we will split the tensor by reshaping it in this dimension\n",
        "        target_shape = (batch_size,-1,self.num_heads,self.dim_heads)\n",
        "\n",
        "        # split the input\n",
        "        splitted_input = tf.reshape(input,shape=target_shape)\n",
        "\n",
        "        # once splitted we will reshape is to the required dimension\n",
        "        fin_splitted_input = tf.transpose(splitted_input,perm=[0,2,1,3])\n",
        "\n",
        "        return fin_splitted_input\n",
        "\n",
        "    def call(self,query,key,value,mask):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            query = The query vector        shape = (batch_size,seq_len,d_model)\n",
        "            key   = The key   vector        shape = (batch_size,seq_len,d_model)\n",
        "            value = The value vector        shape = (batch_size,seq_len,d_model)\n",
        "            mask  = A optional matrix to apply mask before computing softmax        shape = (batch_size,1,seq_len,1)\n",
        "        Returns:\n",
        "            Attention(Q,K,V) as defined by the above formula\n",
        "        \"\"\"\n",
        "\n",
        "        # get batch_size\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # passing through linear layer\n",
        "        query = self.linear_q(query)\n",
        "        key   = self.linear_k(key)\n",
        "        value = self.linear_v(value)\n",
        "\n",
        "        # splitting the query key and value\n",
        "        query = self.split_mat(query,batch_size)\n",
        "        key   = self.split_mat(key,batch_size)\n",
        "        value = self.split_mat(value,batch_size)\n",
        "\n",
        "        # apply attention on them\n",
        "        attention = scaled_dot_product_attention(query=query,key=key,value=value,mask=mask)\n",
        "\n",
        "        # getting it in the shape (batch_size,seq_len,num_heads,dim_heads) for concatenation\n",
        "        attentions = tf.transpose(attention,perm=[0,2,1,3])\n",
        "\n",
        "        # concatenating the attention\n",
        "        concat_attention = tf.reshape(attentions,shape=[batch_size,-1,self.d_model])\n",
        "\n",
        "        # pass it through final linear layer\n",
        "        outputs = self.linear_fin(concat_attention)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "9ayXtVJ-JFpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "nGjQP98Rq3Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder Layer"
      ],
      "metadata": {
        "id": "PzwwcaGurBqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder layer is just a single layer of encoder that can be repeated multiple times to make a deep encoder layer. <br>\n",
        "It contains the layers in the following manner.\n",
        "<ul>\n",
        "    <li>MultiHeaded Attention</li>\n",
        "    <li>Dropout</li>\n",
        "    <li>LayerNormalization</li>\n",
        "    <li>Dense with Relu</li>\n",
        "    <li>Dense</li>\n",
        "    <li>Dropout</li>\n",
        "    <li>LayerNormalization</li>\n",
        "</ul>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\"> </p> "
      ],
      "metadata": {
        "id": "r_9BBA54rG9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self,FFN_units,num_heads,dropout):\n",
        "        super(EncoderLayer,self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout   = dropout\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        # get embedding dim\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # attention\n",
        "        self.multihead_attention = MultiHeadedAttention(self.num_heads)\n",
        "\n",
        "        # residual connection\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1     = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,activation='relu')\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "\n",
        "        # residual connection\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2     = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self,inputs,mask,training):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "            inputs   : Input to the encoder layer                       shape = (batch_size,seq_len,d_model)\n",
        "            mask     : Mask that needs to be applied                    shape = ()\n",
        "            training : True or False denoting if it's training or not\n",
        "\n",
        "        Returns\n",
        "            Result after passing through the encoderlayer\n",
        "        \"\"\"\n",
        "        attention = self.multihead_attention(inputs,inputs,inputs,mask)\n",
        "        attention = self.dropout_1(attention,training=training)\n",
        "\n",
        "        # add and normalise\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "\n",
        "        # feed forward network\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs,training=training)\n",
        "\n",
        "        # add and normalise\n",
        "        outputs = self.norm_2(outputs+attention)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "33_6vBJXrAZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder"
      ],
      "metadata": {
        "id": "M89Qe9PB1Yf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder is the complete encoder layer of the transformer and it contains layers stacked in the following manner.\n",
        "<ul>\n",
        "    <li> Embedding </li>\n",
        "    <li> POS Encoding </li>\n",
        "    <li> Dropout </li>\n",
        "    <li> EncoderLayers </li>\n",
        "</ul>\n",
        "<p align=\"center\"><img src=\"https://www.factored.ai/wp-content/uploads/2021/09/image2-580x1024.png\"></p>"
      ],
      "metadata": {
        "id": "tCmp_Ztq1dYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "    def __init__(self,num_layers,FFN_units,num_heads,dropout,vocab_size,d_model,name='Encoder'):\n",
        "        super(Encoder,self).__init__(name=name)\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model    = d_model\n",
        "\n",
        "        self.embedding    = layers.Embedding(vocab_size,d_model)\n",
        "        self.pos_encoding = PositionalEmbedding()\n",
        "        self.dropout      = layers.Dropout(rate=dropout)\n",
        "\n",
        "        # Put the multiple encoding layer in a list\n",
        "        self.enc_layers   = [EncoderLayer(FFN_units,num_heads,dropout) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self,inputs,mask,training):\n",
        "        outputs = self.embedding(inputs)\n",
        "\n",
        "        # multiply it by sqrt(d_model) as suggested in paper\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model,tf.float32))    \n",
        "\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs,training=training)\n",
        "\n",
        "        # pass output through all the encoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            outputs = self.enc_layers[i](outputs,mask,training)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "tScMNfYA1bqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "d5PMoJGL6ghD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder Layer"
      ],
      "metadata": {
        "id": "tAmr2fjy6mKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we create the decoder layer that is the building block for the decoder we will be building. <br>\n",
        "It contains the following layers stacked in this fashion\n",
        "<ul>\n",
        "    <li> MultiHeadAttention with look ahead mask </li>\n",
        "    <li> Dropout </li>\n",
        "    <li> LayerNormalization </li>\n",
        "    <li> MultiHeadAttention </li>\n",
        "    <li> Dropout </li>\n",
        "    <li> LayerNormalization </li>\n",
        "    <li> Dense with ReLU </li>\n",
        "    <li> Dense</li>\n",
        "    <li> Dropout </li>\n",
        "    <li> LayerNormalization </li>\n",
        "</ul>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.stack.imgur.com/kOs4Z.png\"></p>"
      ],
      "metadata": {
        "id": "rUsGQT1j6o1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self,FFN_units,num_heads,dropout):\n",
        "        super(DecoderLayer,self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout   = dropout\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        # getting embd dim\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # MultiHead Attention with look ahead mask\n",
        "        self.multi_headed_attention_1 = MultiHeadedAttention(self.num_heads)\n",
        "        self.dropout_1                = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1                   = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # MultiHead Attention\n",
        "        self.multi_headed_attention_2 = MultiHeadedAttention(self.num_heads)\n",
        "        self.dropout_2                = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2                   = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # FeedForward network\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,activation='relu')\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "\n",
        "        # residual connection\n",
        "        self.dropout_3  = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_3     = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self,inputs,enc_outputs,mask_1,mask_2,training):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            inputs : Input to the encoder\n",
        "            enc_outputs : Output from the decoder\n",
        "            mask_1 : look ahead mask\n",
        "            mask_2 : mask for sencond multihead attention\n",
        "            training : boolean denoting if it is training\n",
        "        \"\"\"\n",
        "        # first attention\n",
        "        attention = self.multi_headed_attention_1(inputs,inputs,inputs,mask_1)\n",
        "        attention = self.dropout_1(attention,training=training)\n",
        "\n",
        "        # add and norm\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "\n",
        "        # second attention\n",
        "        attention_2 = self.multi_headed_attention_2(attention,enc_outputs,enc_outputs,mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2,training=training)\n",
        "\n",
        "        # add and norm\n",
        "        attention_2 = self.norm_2(attention+attention_2)\n",
        "\n",
        "        # feed forward\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "\n",
        "        # residual connection\n",
        "        outputs = self.dropout_3(outputs,training=training)\n",
        "\n",
        "        # add and norm\n",
        "        outputs = self.norm_3(outputs+attention_2)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "QG9YBsgt87tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder"
      ],
      "metadata": {
        "id": "dzz5zTXpC1Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the final decoder layer and it consist of the following layers stacked together.\n",
        "<ul>\n",
        "    <li> Embedding </li>\n",
        "    <li> POS Encoding </li>\n",
        "    <li> Dropout </li>\n",
        "    <li> EncoderLayers </li>\n",
        "</ul>\n",
        "<p align=\"center\"><img src=\"https://miro.medium.com/max/1042/1*AOQlFGXhx_ojDcEX1stg2g.png\"> </p>"
      ],
      "metadata": {
        "id": "OELhNjy-C3Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(layers.Layer):\n",
        "    def __init__(self,num_layers,FFN_units,num_heads,dropout,vocab_size,d_model,name='Decoder'):\n",
        "        super(Decoder,self).__init__(name=name)\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model    = d_model\n",
        "\n",
        "        self.embedding    = layers.Embedding(vocab_size,d_model)\n",
        "        self.pos_encoding = PositionalEmbedding()\n",
        "        self.dropout      = layers.Dropout(rate=dropout)\n",
        "\n",
        "        # Put the multiple encoding layer in a list\n",
        "        self.dec_layers   = [DecoderLayer(FFN_units,num_heads,dropout) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self,inputs,enc_outputs,mask_1,mask_2,training):\n",
        "        outputs = self.embedding(inputs)\n",
        "\n",
        "        # multiply it by sqrt(d_model) as suggested in paper\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model,tf.float32))    \n",
        "\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs,training=training)\n",
        "\n",
        "        # pass output through all the encoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            outputs = self.dec_layers[i](outputs,enc_outputs,mask_1,mask_2,training)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ELQqhOTODeFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "LZztwP96Er6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have already built the components of transformer, building the transformer is easy. We just need to pass the final output of decoder through a dense layer to get desired output.\n",
        "<p align=\"center\"><img src=\"https://miro.medium.com/max/400/1*tkC_NqylGH4hv2rTLPOf5A.png\"></p>"
      ],
      "metadata": {
        "id": "0FGV_sbbEy1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 num_layers,\n",
        "                 FFN_units,\n",
        "                 num_heads,\n",
        "                 dropout,\n",
        "                 name='Transformer'):\n",
        "        super(Transformer,self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(num_layers,FFN_units,num_heads,dropout,vocab_size_enc,d_model)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(num_layers,FFN_units,num_heads,dropout,vocab_size_dec,d_model)\n",
        "\n",
        "        # Dense Layer\n",
        "        self.fin_linear = layers.Dense(units=vocab_size_dec)\n",
        "\n",
        "    def create_padding_mask(self,seq):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            seq : A sequence of shape (batch_size,seq)\n",
        "        Returns:\n",
        "            A mask to mask all the paddings    \n",
        "        \"\"\"\n",
        "        mask = tf.cast(tf.math.equal(seq,0),tf.float32)\n",
        "        return mask[:,tf.newaxis,tf.newaxis,:]\n",
        "\n",
        "    def create_look_ahead_mask(self,seq):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            seq : A sequence of shape (batch_size,seq)\n",
        "        Returns:\n",
        "            A mask to mask all the future values    \n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0)\n",
        "        return look_ahead_mask\n",
        "\n",
        "    def call(self,enc_inputs,dec_inputs,training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),self.create_look_ahead_mask(dec_inputs))\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs,enc_mask,training)\n",
        "        dec_outputs = self.decoder(dec_inputs,enc_outputs,dec_mask_1,dec_mask_2,training)\n",
        "        outputs = self.fin_linear(dec_outputs)\n",
        "\n",
        "        return outputs\n",
        "    "
      ],
      "metadata": {
        "id": "zKnAWu4EEyLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "nlv4psz0efgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Preparation"
      ],
      "metadata": {
        "id": "zDt1IBYTfmHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare constants\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Parameter = value used here # value used in paper\n",
        "\n",
        "D_MODEL    = 128   # 512\n",
        "NUM_LAYERS = 4     # 6\n",
        "NUM_HEADS  = 8     # 8\n",
        "FFN_UNITS  = 512   # 2048\n",
        "DROPOUT    = 0.1   # 0.1"
      ],
      "metadata": {
        "id": "wFSwT_RreoeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "transformer = Transformer(\n",
        "    vocab_size_enc = VOCAB_SIZE_EN,\n",
        "    vocab_size_dec = VOCAB_SIZE_FR,\n",
        "    d_model        = D_MODEL,\n",
        "    num_layers     = NUM_LAYERS,\n",
        "    FFN_units      = FFN_UNITS,\n",
        "    num_heads      = NUM_HEADS,\n",
        "    dropout        = DROPOUT\n",
        ")"
      ],
      "metadata": {
        "id": "d1w5bOuFfjtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom loss function"
      ],
      "metadata": {
        "id": "IE18QsR_gSo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The loss we will be using\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(target,pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target,0))\n",
        "    loss_ = loss_object(target,pred)\n",
        "    mask = tf.cast(mask,dtype=loss_.dtype)\n",
        "    loss_*=mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train-loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train-accuracy')"
      ],
      "metadata": {
        "id": "pDtTuPbBgWmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Learning Rate"
      ],
      "metadata": {
        "id": "JUF6_2lRiE0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self,d_model,warmup_steps=4000):\n",
        "        super(CustomSchedule,self).__init__()\n",
        "        self.d_model = tf.cast(d_model,tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self,step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step*(self.warmup_steps**-1.5)\n",
        "        return tf.math.rsqrt(self.d_model)*tf.math.minimum(arg1,arg2)\n"
      ],
      "metadata": {
        "id": "Ku8iYolOiQtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Optimier\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1 = 0.9,\n",
        "    beta_2 = 0.98,\n",
        "    epsilon = 1e-9\n",
        ")"
      ],
      "metadata": {
        "id": "wqPsrpaljcH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (enc_inputs,targets) in dataset:\n",
        "    dec_inputs = targets[:,:-1]\n",
        "    dec_outputs = targets[:,1:]\n",
        "    predictions = transformer(enc_inputs,dec_inputs,True)\n",
        "    break"
      ],
      "metadata": {
        "id": "4PfK6n6wCHEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CD5TAAAE48-",
        "outputId": "d7caa725-9d8d-4be0-83c3-8de970f8147a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Encoder (Encoder)           multiple                  1841408   \n",
            "                                                                 \n",
            " Decoder (Decoder)           multiple                  2100864   \n",
            "                                                                 \n",
            " dense_130 (Dense)           multiple                  1050705   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,992,977\n",
            "Trainable params: 4,992,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.trainable_variables[1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzFFpP8NE_b5",
        "outputId": "28de68ef-4b07-4e8c-b6cb-ca0db3410859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'transformer_2/Encoder/encoder_layer_8/multi_headed_attention_24/dense_133/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
              "array([[-0.10447738,  0.12347047, -0.12454616, ...,  0.08555354,\n",
              "        -0.0239334 , -0.00110282],\n",
              "       [-0.11210363, -0.12163067, -0.11345045, ..., -0.11269811,\n",
              "        -0.10498777,  0.10983653],\n",
              "       [ 0.13472046,  0.14327098,  0.03912929, ..., -0.07259563,\n",
              "        -0.02081156, -0.15288371],\n",
              "       ...,\n",
              "       [ 0.06775694, -0.14836188,  0.09104872, ...,  0.06240706,\n",
              "         0.01480077, -0.03298108],\n",
              "       [-0.0678867 ,  0.00817269,  0.11684592, ...,  0.01207483,\n",
              "         0.08595143, -0.13826331],\n",
              "       [ 0.12163402, -0.00171679, -0.0451908 , ...,  0.10564192,\n",
              "         0.04345226, -0.04623508]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checkpoints to save our model"
      ],
      "metadata": {
        "id": "lQIymz-ej4fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(BASE_DIR,'Models')\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint_path,max_to_keep=3)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    status = ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint Restored !!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N8J4-kyrj33Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeca3722-eeb3-4c80-f429-58e8126d31a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Checkpoint Restored !!\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.trainable_variables[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RML8BbwkFeXW",
        "outputId": "d498cf5a-f14e-492b-b6c0-421db8b80d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'transformer_2/Encoder/encoder_layer_8/multi_headed_attention_24/dense_133/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
              "array([[-0.11415039, -0.10285144,  0.14213888, ..., -0.07803464,\n",
              "         0.12838785, -0.06542238],\n",
              "       [-0.14188728,  0.0145061 ,  0.00297123, ...,  0.12856068,\n",
              "        -0.05406941,  0.07711782],\n",
              "       [ 0.01665758, -0.12875646, -0.0235927 , ..., -0.03335512,\n",
              "        -0.06182152,  0.05719118],\n",
              "       ...,\n",
              "       [ 0.12384088,  0.00334989,  0.04763618, ...,  0.00324349,\n",
              "        -0.00252198,  0.08868234],\n",
              "       [ 0.03039664, -0.12679702,  0.01593955, ...,  0.02875179,\n",
              "        -0.150753  ,  0.01999334],\n",
              "       [-0.07684312, -0.1381748 , -0.09567983, ..., -0.05383786,\n",
              "        -0.03598132,  0.14608754]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "YZTQJL9WlO5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Start of epoch {epoch+1}\")\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for batch,(enc_inputs,targets) in tq.tqdm(enumerate(dataset)):\n",
        "        dec_inputs = targets[:,:-1]\n",
        "        dec_outputs = targets[:,1:]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs,dec_inputs,True)\n",
        "            loss = loss_function(dec_outputs,predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss,transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients,transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs,predictions)\n",
        "\n",
        "        if batch%50==0:\n",
        "            print(f\"Epoch : {epoch+1} | Batch : {batch} | Loss : {train_loss.result()} | Accuracy : {train_accuracy.result()}\")\n",
        "\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "\n",
        "    print(f\"Saving checkpoint for epoch {epoch} at {ckpt_save_path}\")\n",
        "    print(f\"Time taken for this epoch : {time.time()-start} sec\")"
      ],
      "metadata": {
        "id": "kMy3mcRclRhA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "a5605b939ee847a9a5c4a1e2078ed131",
            "c9a5e2a8a7ae4253a7db9fda9e6c1db8",
            "ee8c194a93104e33b15d925df202c9ce",
            "aa22c08e7bff4ed19391b677ffaa2732",
            "2f8bfad53a974023ab5d0a7101445ce6",
            "9c4bda9e63ff46b3af01875842da2bdf",
            "a200ccd3960f4a0eb34bfcfbf8b10e52",
            "dbf3dabc44e4499e90918a5c3006f72d",
            "73de936feaaa496d868a668bd0e4c431",
            "5e12b7d8ebef4b5282acc54463cf0f41",
            "df6f6ef2b9c24681a3965ceda51d882e"
          ]
        },
        "outputId": "1d61ffd1-404b-4ed5-f1fd-cd15ad3a5903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5605b939ee847a9a5c4a1e2078ed131",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 | Batch : 0 | Loss : 6.172541618347168 | Accuracy : 0.0008223684271797538\n",
            "Epoch : 1 | Batch : 50 | Loss : 6.007894515991211 | Accuracy : 0.14501096308231354\n",
            "Epoch : 1 | Batch : 100 | Loss : 5.854761600494385 | Accuracy : 0.23225800693035126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "U6pYP-v3pCRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "\n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2],axis=0)\n",
        "\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input,output,False)\n",
        "        prediction = predictions[:,-1:,:]\n",
        "\n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output,axis=0)\n",
        "\n",
        "        output = tf.concat([output,predicted_id],axis=-1)\n",
        "\n",
        "    return tf.squeeze(output,axis=0)"
      ],
      "metadata": {
        "id": "l9CN_rD4pErl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentance):\n",
        "    output = evaluate(sentance).numpy()\n",
        "    predicted_sentance = tokenizer_fr.decode([i for i in output if i<VOCAB_SIZE_FR-2])\n",
        "    print(f\"Input sentance = {sentance}\")\n",
        "    print(f\"Predicted translation = {predicted_sentance}\")"
      ],
      "metadata": {
        "id": "ujJvqkC8qmoI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}